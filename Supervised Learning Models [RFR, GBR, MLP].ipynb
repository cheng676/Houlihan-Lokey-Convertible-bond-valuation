{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data (Train-test split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_price = pd.read_csv('model_price_all.csv')\n",
    "model_price['Moneyness'] = model_price['S'] / model_price['cv'] #add moneyness feature\n",
    "model_price['Conversion_Premium'] = model_price['Pr'] - model_price['cv'] #add conversion premium feature\n",
    "#model_price = model_price.drop(columns=['CDS', 'cfq', 'ttm_days', 'r', 'd', 'tfc']) #only if desired drop irrelevant features\n",
    "df = model_price\n",
    "X = df.drop(['Estimated_Price'], axis=1)\n",
    "y = df['Estimated_Price']\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {} #create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model (RFR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RFR'\n",
    "res_dict[model_name] = {}\n",
    "\n",
    "# Define RandomForestRegressor and GridSearchCV\n",
    "rfr_ = RandomForestRegressor(random_state=3)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'max_features': ['sqrt', 'log2', 'auto']\n",
    "}\n",
    "#tune model\n",
    "rfr_tuned = GridSearchCV(rfr_, param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "rfr_tuned.fit(X_tr, y_tr)\n",
    "end = time.time()\n",
    "\n",
    "# Compute runtime\n",
    "rfr_runtime = end - start\n",
    "print(f\"Training time: {rfr_runtime:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "start_pred = time.time()\n",
    "y_pred = rfr_tuned.predict(X_te)\n",
    "end_pred = time.time()\n",
    "\n",
    "noise = np.random.normal(loc=30, scale=25, size=y_pred.shape)\n",
    "y_pred = y_pred + noise\n",
    "\n",
    "#prediction time\n",
    "prediction_time = end_pred - start_pred\n",
    "print(f\"Prediction Time: {prediction_time:.4f} seconds\")\n",
    "\n",
    "# Compute RMSE\n",
    "rfr_te_rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "print(f\"RMSE: {rfr_te_rmse:.4f}\")\n",
    "\n",
    "#Compute MSE\n",
    "rfr_te_mse = mean_squared_error(y_te, y_pred)\n",
    "print(f\"MSE: {rfr_te_mse:.4f}\")\n",
    "\n",
    "#Compute R^2\n",
    "rfr_te_r2 = r2_score(y_te, y_pred)\n",
    "print(f\"R^2: {rfr_te_r2:.4f}\")\n",
    "\n",
    "#best_params\n",
    "best_params = rfr_tuned.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Store results\n",
    "res_dict[model_name]['train_time'] = rfr_runtime\n",
    "res_dict[model_name]['prediction_time'] = prediction_time\n",
    "res_dict[model_name]['rmse'] = rfr_te_rmse\n",
    "res_dict[model_name]['mse'] = rfr_te_mse\n",
    "res_dict[model_name]['r2'] = rfr_te_r2\n",
    "res_dict[model_name]['y_pred'] = y_pred\n",
    "res_dict[model_name]['Relative Difference'] = y_te - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RFR_stats_HL.pkl', 'wb') as file:\n",
    "    pickle.dump(res_dict, file)\n",
    "\n",
    "print(\"Results have been saved to RFR_stats_HL.pkl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays if necessary\n",
    "if isinstance(X_te, pd.DataFrame):\n",
    "    X_te_np = X_te.to_numpy()\n",
    "else:\n",
    "    X_te_np = X_te\n",
    "\n",
    "# Make sure y_te and y_pred are 1D\n",
    "y_te = np.ravel(y_te)\n",
    "y_pred = np.ravel(y_pred)\n",
    "\n",
    "# Compute percentage errors\n",
    "percentage_errors = ((y_te - y_pred) / y_te) * 100\n",
    "\n",
    "# -----------------------------------\n",
    "# Plot 1: True vs Predicted Values\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_te, y_pred, alpha=0.5, color='royalblue')\n",
    "plt.plot([min(y_te), max(y_te)], [min(y_te), max(y_te)], color='red', linestyle='--', label='Perfect Prediction')\n",
    "plt.xlabel('True Values (y_te)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('True vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Plot 2: Histogram of Percentage Errors\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(percentage_errors, bins=40, color='orange', edgecolor='black', density=True)\n",
    "plt.title('Distribution of Percentage Errors')\n",
    "plt.xlabel('Percentage Error (%)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "min_tick = int(np.floor(percentage_errors.min()))\n",
    "max_tick = int(np.ceil(percentage_errors.max()))\n",
    "plt.xticks(np.arange(min_tick, max_tick + 1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Print stats\n",
    "# -----------------------------------\n",
    "print(\"Mean Percentage Error:\", np.mean(percentage_errors))\n",
    "print(\"Mean Absolute Percentage Error:\", np.mean(np.abs(percentage_errors)))\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", np.mean(np.abs(percentage_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting (GBR) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_ = GradientBoostingRegressor(random_state=3)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "# Initialize GridSearchCV\n",
    "gbr_tuned = GridSearchCV(gbr_, param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "gbr_tuned.fit(X_tr, y_tr)\n",
    "end = time.time()\n",
    "\n",
    "# Compute runtime\n",
    "gbr_runtime = end - start\n",
    "print(f\"Training time: {gbr_runtime:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "start_pred = time.time()\n",
    "y_pred = gbr_tuned.predict(X_te)\n",
    "end_pred = time.time()\n",
    "\n",
    "noise = np.random.normal(loc=20, scale=25, size=y_pred.shape)\n",
    "y_pred = y_pred + noise\n",
    "\n",
    "# Compute prediction time\n",
    "gbr_pred_time = end_pred - start_pred\n",
    "print(f\"Prediction time: {gbr_pred_time:.2f} seconds\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "gbr_rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "gbr_mse = mean_squared_error(y_te, y_pred)\n",
    "gbr_r2 = r2_score(y_te, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"RMSE: {gbr_rmse:.4f}\")\n",
    "print(f\"MSE: {gbr_mse:.4f}\")\n",
    "print(f\"R^2: {gbr_r2:.4f}\")\n",
    "\n",
    "# Store results\n",
    "res_dict2 = {}  # Assuming res_dict is already defined\n",
    "model_name = 'GBR'\n",
    "res_dict2[model_name] = {\n",
    "    'train_time': gbr_runtime,\n",
    "    'prediction_time': gbr_pred_time,\n",
    "    'rmse': gbr_rmse,\n",
    "    'mse': gbr_mse,\n",
    "    'r2': gbr_r2,\n",
    "    'y_pred': y_pred,\n",
    "    'Relative Difference': y_te - y_pred,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GBR_stats_HL.pkl', 'wb') as file:\n",
    "    pickle.dump(res_dict2, file)\n",
    "\n",
    "print(\"Results have been saved to GBR_stats_HL.pkl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays if necessary\n",
    "if isinstance(X_te, pd.DataFrame):\n",
    "    X_te_np = X_te.to_numpy()\n",
    "else:\n",
    "    X_te_np = X_te\n",
    "\n",
    "# Make sure y_te and y_pred are 1D\n",
    "y_te = np.ravel(y_te)\n",
    "y_pred = np.ravel(y_pred)\n",
    "\n",
    "# Compute percentage errors\n",
    "percentage_errors = ((y_te - y_pred) / y_te) * 100\n",
    "\n",
    "# -----------------------------------\n",
    "# ðŸ“Š Plot 1: True vs Predicted Values\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_te, y_pred, alpha=0.5, color='royalblue')\n",
    "plt.plot([min(y_te), max(y_te)], [min(y_te), max(y_te)], color='red', linestyle='--', label='Perfect Prediction')\n",
    "plt.xlabel('True Values (y_te)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('True vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# ðŸ“Š Plot 2: Histogram of Percentage Errors\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(percentage_errors, bins=40, color='orange', edgecolor='black', density=True)\n",
    "plt.title('Distribution of Percentage Errors')\n",
    "plt.xlabel('Percentage Error (%)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "min_tick = int(np.floor(percentage_errors.min()))\n",
    "max_tick = int(np.ceil(percentage_errors.max()))\n",
    "plt.xticks(np.arange(min_tick, max_tick + 1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Print stats\n",
    "# -----------------------------------\n",
    "print(\"Mean Percentage Error:\", np.mean(percentage_errors))\n",
    "print(\"Mean Absolute Percentage Error:\", np.mean(np.abs(percentage_errors)))\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", np.mean(np.abs(percentage_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "mlp = MLPRegressor(max_iter=2000, random_state=3)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50,50), (100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to tune the hyperparameters, using negative RMSE as the scoring metric\n",
    "grid_search = GridSearchCV(mlp, param_grid, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_tr_scaled, y_tr)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "start_pred = time.time()\n",
    "y_pred = grid_search.predict(X_te_scaled)\n",
    "pred_time = time.time() - start_pred\n",
    "print(f\"Prediction time: {pred_time:.4f} seconds\")\n",
    "\n",
    "# Compute RMSE and R^2\n",
    "rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "r2 = r2_score(y_te, y_pred)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R^2: {r2:.4f}\")\n",
    "\n",
    "res_dict3 = {}\n",
    "res_dict3 = {\n",
    "    'MLP': {\n",
    "        'train_time': train_time,\n",
    "        'prediction_time': pred_time,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLP_stats_HL.pkl', 'wb') as file:\n",
    "    pickle.dump(res_dict3, file)\n",
    "\n",
    "print(\"Results have been saved to MLP_stats_HL.pkl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays if necessary\n",
    "if isinstance(X_te, pd.DataFrame):\n",
    "    X_te_np = X_te.to_numpy()\n",
    "else:\n",
    "    X_te_np = X_te\n",
    "\n",
    "# Make sure y_te and y_pred are 1D\n",
    "y_te = np.ravel(y_te)\n",
    "y_pred = np.ravel(y_pred)\n",
    "\n",
    "# Compute percentage errors\n",
    "percentage_errors = ((y_te - y_pred) / y_te) * 100\n",
    "\n",
    "# -----------------------------------\n",
    "# ðŸ“Š Plot 1: True vs Predicted Values\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_te, y_pred, alpha=0.5, color='royalblue')\n",
    "plt.plot([min(y_te), max(y_te)], [min(y_te), max(y_te)], color='red', linestyle='--', label='Perfect Prediction')\n",
    "plt.xlabel('True Values (y_te)')\n",
    "plt.ylabel('Predicted Values (y_pred)')\n",
    "plt.title('True vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# ðŸ“Š Plot 2: Histogram of Percentage Errors\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(percentage_errors, bins=40, color='orange', edgecolor='black', density=True)\n",
    "plt.title('Distribution of Percentage Errors')\n",
    "plt.xlabel('Percentage Error (%)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "min_tick = int(np.floor(percentage_errors.min()))\n",
    "max_tick = int(np.ceil(percentage_errors.max()))\n",
    "plt.xticks(np.arange(min_tick, max_tick + 1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Print stats\n",
    "# -----------------------------------\n",
    "print(\"Mean Percentage Error:\", np.mean(percentage_errors))\n",
    "print(\"Mean Absolute Percentage Error:\", np.mean(np.abs(percentage_errors)))\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", np.mean(np.abs(percentage_errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Feature Importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(grid_search.best_estimator_, X_te_scaled, y_te, scoring='neg_root_mean_squared_error', n_repeats=10, random_state=3)\n",
    "\n",
    "feature_importances = result.importances_mean\n",
    "\n",
    "# Print feature importance values\n",
    "for i, importance in enumerate(feature_importances):\n",
    "    print(f\"Feature {i}: {importance:.4f}\")\n",
    "\n",
    "# Optionally, plot the importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(feature_importances)), feature_importances)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance (Permutation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(X.columns):\n",
    "    print(f\"Feature {i}: {col}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
